{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size: 37px\">Phases</p>\n",
    "<br>\n",
    "<ul style = \"font-size : 25px;\">\n",
    "<li ><a href = \"#Loading the data\">Loading the data</a></li>\n",
    "<br/>    \n",
    "<li><a href = \"#Data preprocessing\">Data preprocessing</a></li>\n",
    "<br/>      \n",
    "<li><a href = \"#Model Building\">Model Building</a></li>       \n",
    "</ul>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "<p id = \"Loading the data\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                                  # use regular expression to deal with text\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize     # for word and sentence tokenization\n",
    "from nltk.corpus import stopwords                          # to help removing stopping words\n",
    "from nltk.stem import WordNetLemmatizer                    # to lemmatize the words\n",
    "from string import punctuation                             # to remove punctuation like ? ! . etc\n",
    "import io                                                  # to read embeddings from text file\n",
    "import math                                                # to get mathmatical functions sin cos etc\n",
    "import pandas as pd                                        # to deal with dataframes    \n",
    "import numpy as np                                         # to deal with numbers and dataframes  \n",
    "import tensorflow as tf                                    # the main package to build the transformer\n",
    "import sys                                                 # good printing tool\n",
    "import pickle                                              # importing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red the data as datafram\n",
    "df = pd.read_csv(r\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the embeddings as dictionary where key is the word and value it's corresponding embbeding\n",
    "def load_vectors(fname):\n",
    "    \"\"\"\n",
    "    this function takes the path of the embeddings file 'must be from fast text'\n",
    "    and returns a dictionary where the words are keys and thier embeddings as values\n",
    "    input :\n",
    "    fname (String) ------> file path\n",
    "    output :\n",
    "    data (dictionary)----> dictionary to acess the embeddings with words as keys\n",
    "    \"\"\"\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] =  tokens[1:] \n",
    "    # add pad token as zero vector\n",
    "    # we will expalain this later\n",
    "    data[\"<PAD>\"] = [0]*300    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = load_vectors(\"wiki.simple.vec.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "<p id = \"Data preprocessing\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleansing(text,maximum_len = 20):\n",
    "    \"\"\"\n",
    "     This function performs the following steps:\n",
    "\n",
    "    1. Lowercases the text\n",
    "    2. Removes HTTP and WWW links\n",
    "    3. Removes non-ASCII characters\n",
    "    4. Removes numbers\n",
    "    5. Tokenizes the text\n",
    "    6. Removes stop words\n",
    "    7. Lemmatizes the tokens\n",
    "    8. Truncates tokens that are longer than the specified maximum length\n",
    "    inputs :\n",
    "    text (String) -----------> String to perform the cleaning\n",
    "    maximum_len(int)--------->The maximum length of tokens to keep\n",
    "    output:\n",
    "    cleaned_tokens(text)----->A string containing the cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "    # lower case the text\n",
    "    text = text.lower()\n",
    "    # replace the text that have a ppatern of http link with empty link\n",
    "    text = re.sub(r\"(https?[\\s+]?:?[\\s+]?[(\\w+|.)/?(\\w+|.)?]+[\\s]?)\",\"\",text)\n",
    "    # replace the text that have a ppatern of only www. link\n",
    "    text = re.sub(r\"(\\w+[\\s+]?:[\\s+]?www.\\w+.\\w+)\",\"\",text)\n",
    "    # remove none ASCII characters like \"| ¡ ¿ † ‡ ↔ ↑ ↓ • ¶\"\n",
    "    text = re.sub(r'[^\\x00-\\x7f]','',text)\n",
    "    # remove numbers from the text \n",
    "    text = re.sub(r\"\\d\",\"\",text)\n",
    "    # turn the text into list of words \n",
    "    tokens = word_tokenize(text)\n",
    "    # cach the stopping words that in english\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    # intialize a lemmatizer \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # define a new list to keep the cleaned tokens\n",
    "    cleaned_tokens = []\n",
    "    # iterate over the tokens to remove the words that exceeds the maximum length \n",
    "    # remove stopping words\n",
    "    # remove punctuation\n",
    "    # lemmatize the words\n",
    "    for i,token in enumerate(tokens):\n",
    "        if (len(token) > maximum_len) or (token in stop_words):\n",
    "            # do nothing \n",
    "            s = \"\"\n",
    "        else:\n",
    "            s = token.strip(punctuation)\n",
    "            if s != \"\":\n",
    "                cleaned_tokens.append(lemmatizer.lemmatize(s))\n",
    "\n",
    "    return \" \".join(cleaned_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the claning to each row of the data\n",
    "df.iloc[:,1] = df.iloc[:,1].apply(text_cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    this function truncate the sentences that exceeds 50 words\n",
    "    and add paddings to senteces that is shorter than 50\n",
    "    input :\n",
    "    text (String) ------> text of the sentence\n",
    "    output:\n",
    "    tokens (list) ------> list of words 'tokens'\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)  \n",
    "    if len(tokens) > 50:\n",
    "        tokens = tokens[:50]\n",
    "        return tokens\n",
    "    else :\n",
    "        padding_len = 50 - len(tokens)\n",
    "        tokens.extend([\"<PAD>\"]*padding_len)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the tokenizer to each sentence\n",
    "df.iloc[:,1] = df.iloc[:,1].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "<p id = \"Model Building\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, dic, n = 50, d = 300, h = 2, k = 300, c = 1200, m = 2):\n",
    "        self.dic = dic\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.m = m\n",
    "        self.positional_encoding      = self.positional_embeddings()\n",
    "        self.encoder_parameters       = self.Encoder_parameters_intializer()\n",
    "        self.head_parameters          = self.Head_parameters_intializer()\n",
    "        self.parameters               = self.encoder_parameters + self.head_parameters\n",
    "        \n",
    "    def import_parameters(self,pname):\n",
    "        with open(pname, 'rb') as file:\n",
    "            # Call load method to deserialze\n",
    "            self.parameters = pickle.load(file)\n",
    "        self.encoder_parameters = self.parameters[:8]\n",
    "        self.head_parameters    = self.parameters[8:]\n",
    "        \n",
    "    def input_embeddings(self, sentence):\n",
    "        \"\"\"\n",
    "        Converts a sentence into a matrix of input embeddings of dim d x n\n",
    "        d is embeddings length \n",
    "        n is the sentence length\n",
    "        input :\n",
    "        sentence (list) ------> list of words\n",
    "        dic (dictionary)------> dictionary of embeddings\n",
    "        output :\n",
    "        tokens (Tensor)-------> tensor of size [d , n]\n",
    "\n",
    "        \"\"\"\n",
    "        dic = self.dic\n",
    "        \n",
    "        tokens = []\n",
    "        for word in sentence:\n",
    "            if word in dic:\n",
    "                tokens.append(dic[word])\n",
    "            # if the word out-of-vocab use special token </s>    \n",
    "            else :\n",
    "                tokens.append(dic['</s>'])\n",
    "        # this make sure that the array in size (d,n)        \n",
    "        tokens =  np.array(tokens, dtype = np.float32).T \n",
    "        return tf.Variable(tokens)      \n",
    "    def positional_embeddings(self, c = 10000):\n",
    "        \"\"\"\n",
    "        get the positional embeddings of embeddings of size [d,n]\n",
    "        inputs :\n",
    "        d (int) -------> is the diemnsion of the embeddings\n",
    "        n (int) -------> is the number of tokens in the sentence\n",
    "        c (int) -------> hyperparameter by default 10000\n",
    "        output :\n",
    "        tokens (Tensor)------> positional embeddings for the tokens\n",
    "        \"\"\"\n",
    "        d = self.d\n",
    "        n = self.n\n",
    "        \n",
    "        tokens = []\n",
    "        for position in range(n):\n",
    "            tokens.append([])\n",
    "            for i in range(d):\n",
    "                # if i is even calculate even_pe\n",
    "                if i%2 == 0:\n",
    "                    even_d    = math.pow(c, i/d)\n",
    "                    even_pe = math.sin(position/ even_d)\n",
    "                    tokens[position].append(even_pe)\n",
    "                # if i is odd calculate odd_pe    \n",
    "                else:\n",
    "                    odd_d    = math.pow(c, i/d)\n",
    "                    odd_pe = math.cos(position/ odd_d)\n",
    "                    tokens[position].append(odd_pe)            \n",
    "        return tf.Variable(np.array(tokens).T, dtype = tf.float32, shape = [d,n] )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

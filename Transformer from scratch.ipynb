{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size: 37px\">Phases</p>\n",
    "<br>\n",
    "<ul style = \"font-size : 25px;\">\n",
    "<li ><a href = \"#Loading the data\">Loading the data</a></li>\n",
    "<br/>    \n",
    "<li><a href = \"#Data preprocessing\">Data preprocessing</a></li>\n",
    "<br/>      \n",
    "<li><a href = \"#Model Building\">Model Building</a></li>       \n",
    "</ul>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "<p id = \"Loading the data\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                                  # use regular expression to deal with text\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize     # for word and sentence tokenization\n",
    "from nltk.corpus import stopwords                          # to help removing stopping words\n",
    "from nltk.stem import WordNetLemmatizer                    # to lemmatize the words\n",
    "from string import punctuation                             # to remove punctuation like ? ! . etc\n",
    "import io                                                  # to read embeddings from text file\n",
    "import math                                                # to get mathmatical functions sin cos etc\n",
    "import pandas as pd                                        # to deal with dataframes    \n",
    "import numpy as np                                         # to deal with numbers and dataframes  \n",
    "import tensorflow as tf                                    # the main package to build the transformer\n",
    "import sys                                                 # good printing tool\n",
    "import pickle                                              # importing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red the data as datafram\n",
    "df = pd.read_csv(r\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the embeddings as dictionary where key is the word and value it's corresponding embbeding\n",
    "def load_vectors(fname):\n",
    "    \"\"\"\n",
    "    this function takes the path of the embeddings file 'must be from fast text'\n",
    "    and returns a dictionary where the words are keys and thier embeddings as values\n",
    "    input :\n",
    "    fname (String) ------> file path\n",
    "    output :\n",
    "    data (dictionary)----> dictionary to acess the embeddings with words as keys\n",
    "    \"\"\"\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] =  tokens[1:] \n",
    "    # add pad token as zero vector\n",
    "    # we will expalain this later\n",
    "    data[\"<PAD>\"] = [0]*300    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = load_vectors(\"wiki.simple.vec.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "<p id = \"Data preprocessing\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleansing(text,maximum_len = 20):\n",
    "    \"\"\"\n",
    "     This function performs the following steps:\n",
    "\n",
    "    1. Lowercases the text\n",
    "    2. Removes HTTP and WWW links\n",
    "    3. Removes non-ASCII characters\n",
    "    4. Removes numbers\n",
    "    5. Tokenizes the text\n",
    "    6. Removes stop words\n",
    "    7. Lemmatizes the tokens\n",
    "    8. Truncates tokens that are longer than the specified maximum length\n",
    "    inputs :\n",
    "    text (String) -----------> String to perform the cleaning\n",
    "    maximum_len(int)--------->The maximum length of tokens to keep\n",
    "    output:\n",
    "    cleaned_tokens(text)----->A string containing the cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "    # lower case the text\n",
    "    text = text.lower()\n",
    "    # replace the text that have a ppatern of http link with empty link\n",
    "    text = re.sub(r\"(https?[\\s+]?:?[\\s+]?[(\\w+|.)/?(\\w+|.)?]+[\\s]?)\",\"\",text)\n",
    "    # replace the text that have a ppatern of only www. link\n",
    "    text = re.sub(r\"(\\w+[\\s+]?:[\\s+]?www.\\w+.\\w+)\",\"\",text)\n",
    "    # remove none ASCII characters like \"| ¡ ¿ † ‡ ↔ ↑ ↓ • ¶\"\n",
    "    text = re.sub(r'[^\\x00-\\x7f]','',text)\n",
    "    # remove numbers from the text \n",
    "    text = re.sub(r\"\\d\",\"\",text)\n",
    "    # turn the text into list of words \n",
    "    tokens = word_tokenize(text)\n",
    "    # cach the stopping words that in english\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    # intialize a lemmatizer \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # define a new list to keep the cleaned tokens\n",
    "    cleaned_tokens = []\n",
    "    # iterate over the tokens to remove the words that exceeds the maximum length \n",
    "    # remove stopping words\n",
    "    # remove punctuation\n",
    "    # lemmatize the words\n",
    "    for i,token in enumerate(tokens):\n",
    "        if (len(token) > maximum_len) or (token in stop_words):\n",
    "            # do nothing \n",
    "            s = \"\"\n",
    "        else:\n",
    "            s = token.strip(punctuation)\n",
    "            if s != \"\":\n",
    "                cleaned_tokens.append(lemmatizer.lemmatize(s))\n",
    "\n",
    "    return \" \".join(cleaned_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the claning to each row of the data\n",
    "df.iloc[:,1] = df.iloc[:,1].apply(text_cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    this function truncate the sentences that exceeds 50 words\n",
    "    and add paddings to senteces that is shorter than 50\n",
    "    input :\n",
    "    text (String) ------> text of the sentence\n",
    "    output:\n",
    "    tokens (list) ------> list of words 'tokens'\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)  \n",
    "    if len(tokens) > 50:\n",
    "        tokens = tokens[:50]\n",
    "        return tokens\n",
    "    else :\n",
    "        padding_len = 50 - len(tokens)\n",
    "        tokens.extend([\"<PAD>\"]*padding_len)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the tokenizer to each sentence\n",
    "df.iloc[:,1] = df.iloc[:,1].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "<p id = \"Model Building\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, dic, n = 50, d = 300, h = 2, k = 300, c = 1200, m = 2):\n",
    "        self.dic = dic\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.m = m\n",
    "        self.positional_encoding      = self.positional_embeddings()\n",
    "        self.encoder_parameters       = self.Encoder_parameters_intializer()\n",
    "        self.head_parameters          = self.Head_parameters_intializer()\n",
    "        self.parameters               = self.encoder_parameters + self.head_parameters\n",
    "        \n",
    "    def import_parameters(self,pname):\n",
    "        with open(pname, 'rb') as file:\n",
    "            # Call load method to deserialze\n",
    "            self.parameters = pickle.load(file)\n",
    "        self.encoder_parameters = self.parameters[:8]\n",
    "        self.head_parameters    = self.parameters[8:]\n",
    "        \n",
    "    def input_embeddings(self, sentence):\n",
    "        \"\"\"\n",
    "        Converts a sentence into a matrix of input embeddings of dim d x n\n",
    "        d is embeddings length \n",
    "        n is the sentence length\n",
    "        input :\n",
    "        sentence (list) ------> list of words\n",
    "        dic (dictionary)------> dictionary of embeddings\n",
    "        output :\n",
    "        tokens (Tensor)-------> tensor of size [d , n]\n",
    "\n",
    "        \"\"\"\n",
    "        dic = self.dic\n",
    "        \n",
    "        tokens = []\n",
    "        for word in sentence:\n",
    "            if word in dic:\n",
    "                tokens.append(dic[word])\n",
    "            # if the word out-of-vocab use special token </s>    \n",
    "            else :\n",
    "                tokens.append(dic['</s>'])\n",
    "        # this make sure that the array in size (d,n)        \n",
    "        tokens =  np.array(tokens, dtype = np.float32).T \n",
    "        return tf.Variable(tokens)      \n",
    "    def positional_embeddings(self, c = 10000):\n",
    "        \"\"\"\n",
    "        get the positional embeddings of embeddings of size [d,n]\n",
    "        inputs :\n",
    "        d (int) -------> is the diemnsion of the embeddings\n",
    "        n (int) -------> is the number of tokens in the sentence\n",
    "        c (int) -------> hyperparameter by default 10000\n",
    "        output :\n",
    "        tokens (Tensor)------> positional embeddings for the tokens\n",
    "        \"\"\"\n",
    "        d = self.d\n",
    "        n = self.n\n",
    "        \n",
    "        tokens = []\n",
    "        for position in range(n):\n",
    "            tokens.append([])\n",
    "            for i in range(d):\n",
    "                # if i is even calculate even_pe\n",
    "                if i%2 == 0:\n",
    "                    even_d    = math.pow(c, i/d)\n",
    "                    even_pe = math.sin(position/ even_d)\n",
    "                    tokens[position].append(even_pe)\n",
    "                # if i is odd calculate odd_pe    \n",
    "                else:\n",
    "                    odd_d    = math.pow(c, i/d)\n",
    "                    odd_pe = math.cos(position/ odd_d)\n",
    "                    tokens[position].append(odd_pe)            \n",
    "        return tf.Variable(np.array(tokens).T, dtype = tf.float32, shape = [d,n] )\n",
    "    def padding_mask(self, X):\n",
    "        \"\"\"\n",
    "        Create mask which marks the zero padding values in the input by a 1\n",
    "        this mask is used to force the model to ignore paddings\n",
    "        inputs:\n",
    "        X (Tensor) -------> matrix of embeddings of size [d,n]\n",
    "        n (int)    -------> number of words in the sentences\n",
    "        output:\n",
    "        mask (Tesnor)----> mask with the same size as X\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        \n",
    "        # get boolean vector to indicate position of padding  \n",
    "        mask = tf.math.equal(X, 0)\n",
    "        mask = tf.reduce_all(mask, axis = 0)\n",
    "        # cast the vector from boolean to float32 then rshape it as column vector\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask = tf.reshape(mask, (1,-1))\n",
    "        # repeat that column n times \n",
    "        mask = tf.concat([mask]*n, axis = 0)\n",
    "        return mask\n",
    "    def MHSA(self, X, U_q, U_k, V, W):\n",
    "        \"\"\"\n",
    "        this function return multi head self attention tensor given the parameters and input matrix X\n",
    "        inputs :\n",
    "        X   (Tensor)----> input matrix of size [d,n]\n",
    "        U_q (Tensor)---> matrix of size [h,k,d] is used to get Query matrix\n",
    "        U_k (Tensor)---> matrix of size [h,k,d] is used to get Key matrix\n",
    "        V   (Tensor)---> projection matrix of size [h,d,d]\n",
    "        W   (Tensor)---> weight matrix with size [d,h*d] to project the concatenated heads back to X size\n",
    "        h   (int)------> number of heads\n",
    "        k   (int)------> dimension of Query,Key matrixes\n",
    "        output :\n",
    "        mhsa (Tensor)----> matrix of size [d,n]\n",
    "        \"\"\"\n",
    "        d   = self.d\n",
    "        n   = self.n\n",
    "        h   = self.h\n",
    "        k   = self.k\n",
    "        \n",
    "        Q   = tf.matmul(U_q, X)  \n",
    "        K   = tf.matmul(U_k, X)\n",
    "        m   = self.padding_mask(X) * -1e9 \n",
    "        A   = tf.nn.softmax((tf.matmul(Q, K, transpose_a = True) + m)  / np.sqrt(k), axis= 1)\n",
    "        Y    = V @ X @ A\n",
    "        mhsa = W @ tf.reshape(Y, [h*d,n])\n",
    "        return mhsa\n",
    "    def Add_Norm(self, x1, x2):\n",
    "        \"\"\"\n",
    "        simple function to add two tensors and then normalize the resultant tensor over their rows\n",
    "        inputs :\n",
    "        x1 (Tensor)-----> tensor of size [d,n]\n",
    "        x2 (Tensor)-----> tensor of size [d,n]\n",
    "        output:\n",
    "        Normalized_X (Tenosr)-----> normalized tensor of size [d,n]\n",
    "        \"\"\"\n",
    "        X = x1 + x2\n",
    "        mean = tf.math.reduce_mean(X,axis = 1)\n",
    "        mean = tf.reshape(mean, [-1,1])\n",
    "        std = tf.math.reduce_std(X,axis = 1)\n",
    "        std = tf.reshape(std, [-1,1]) +1e-6\n",
    "        Normalized_X = (X - mean) / std\n",
    "        return Normalized_X\n",
    "    def feed_forward_NN(self,X, W1, W2, b1, b2):\n",
    "        # c approxamatly 4 * d\n",
    "        # in the paper d = 512 and c = 2048\n",
    "        hidden_layer = tf.nn.relu(W1 @ X + b1)\n",
    "        Dropout      = tf.nn.dropout(hidden_layer,0.5)\n",
    "        output       = W2 @ Dropout + b2\n",
    "        return output\n",
    "    def Encoder_parameters_intializer(self):\n",
    "        # this function just intialize encoder parameters\n",
    "        # All the distributions have been chosen based on a lot of experiments\n",
    "        h = self.h\n",
    "        k = self.k\n",
    "        d = self.d\n",
    "        c = self.c\n",
    "        \n",
    "        U_q = tf.random.uniform((h,k,d), minval = 0, maxval = 1)\n",
    "        U_q = tf.Variable(U_q)\n",
    "        U_k = tf.random.uniform((h,k,d), minval = 0, maxval = 1)\n",
    "        U_k = tf.Variable(U_k)\n",
    "        V   = tf.random.uniform((h,d,d), minval = 0, maxval = 1)\n",
    "        V = tf.Variable(V)\n",
    "        W   = tf.random.uniform((d,h*d), minval = 0, maxval = 1)\n",
    "        W = tf.Variable(W)\n",
    "        W1  = tf.random.normal((c,d))\n",
    "        W1 = tf.Variable(W1)\n",
    "        W2  = tf.random.normal((d,c))\n",
    "        W2 = tf.Variable(W2)\n",
    "        b1  = tf.random.uniform((c,1))\n",
    "        b1 = tf.Variable(b1)\n",
    "        b2  = tf.random.uniform((d,1))\n",
    "        b2 = tf.Variable(b2)\n",
    "        return U_q,U_k,V,W,W1,W2,b1,b2\n",
    "    \n",
    "    def Head_parameters_intializer(self,c3 = 6):\n",
    "        # this function just intialize Head parameters\n",
    "        # All the distributions have been chosen based on a lot of experiments\n",
    "        d = self.d\n",
    "        \n",
    "        W_c1 = tf.random.uniform((c3,d), minval = 0, maxval = 1)\n",
    "        W_c1 = tf.Variable(W_c1)\n",
    "        b_c1 = tf.random.uniform((c3,1))\n",
    "        b_c1 = tf.Variable(b_c1)\n",
    "        return W_c1,b_c1\n",
    "    \n",
    "    def Encoder(self, X):\n",
    "        # we have described the function in details through the mathematical notation above\n",
    "        U_q,U_k,V,W,W1,W2,b1,b2 = self.encoder_parameters\n",
    "        X   = X + self.positional_encoding\n",
    "        X    = tf.nn.dropout(X,0.1)\n",
    "        mhsa = self.MHSA(X, U_q, U_k, V, W)\n",
    "        X    = self.Add_Norm(X, mhsa)\n",
    "        fnn  = self.feed_forward_NN(X, W1, W2, b1, b2)\n",
    "        X    = self.Add_Norm(X, fnn)\n",
    "        \n",
    "    def Head(self, X):\n",
    "        # we have described the function in details through the mathematical notation above\n",
    "        W_c1,b_c1 = self.head_parameters\n",
    "        output   = tf.nn.sigmoid(W_c1 @ X + b_c1)\n",
    "        return output\n",
    "        \n",
    "    def Transformer_block(self, X):\n",
    "        \"\"\"\n",
    "        this function combine multiple layers of the encoder to get transformer output\n",
    "        inputs :\n",
    "        X          (Tensor)-----> tesnor of the embeddings of size [d,n]\n",
    "        parameters (tuple)------> tuple of encoder parameters\n",
    "        output:\n",
    "        encoder_output (Tensor)---> the output of the transformer which has the same size as X\n",
    "        \"\"\"\n",
    "        # define one layer outside the loop then use the output as input\n",
    "        # repeat the process m-1 times \n",
    "        # don't forget the dropout to prevent overfitting\n",
    "        encoder_output = self.Encoder(X)\n",
    "        encoder_output = tf.nn.dropout(encoder_output,0.1)\n",
    "        for i in range(self.m-1):\n",
    "            encoder_output = self.Encoder(encoder_output)\n",
    "            encoder_output = tf.nn.dropout(encoder_output,0.1)\n",
    "            \n",
    "    def forward_pass(self, inputs):\n",
    "        \"\"\"\n",
    "        do a forward pass using the transformer architecture and the Head then return the labels\n",
    "        inputs :\n",
    "        inputs (Tensor) ------------> tensor of shape [d,n] that represents a single sentence\n",
    "        encoder_parameters(tuple)---> tuple of U_q,U_k,V,W,W1,W2,b1 and b2 respectively\n",
    "        head_parameters (tuple)-----> tuple of W_c1 and b_c1 respectively\n",
    "        output:\n",
    "        y_hat (tensor)--------------> tesnor of size 6 represents predicted labels\n",
    "        \"\"\"\n",
    "        X     = self.Transformer_block(inputs)\n",
    "        # average all the states to feed it into the head\n",
    "        X     = tf.reshape(tf.reduce_mean(X, axis = 1,),[-1,1]) \n",
    "        y_hat = self.Head(X)\n",
    "        return y_hat\n",
    "               \n",
    "    def predict(self, X_batches):\n",
    "        \"\"\"\n",
    "        predict the outputs given the inputs \n",
    "        input :\n",
    "        X_batches (list)-------> list of tensors each tensor has size [d,n]\n",
    "        output:\n",
    "        Y_hat     (Tensor)-----> Tensor of outputs has size [batch_size,6]\n",
    "        \"\"\"\n",
    "        # define list to store all Y_hat values to concatenate them later\n",
    "        Y_hat = []\n",
    "        # loop over each tensor \n",
    "        for batch in X_batches:\n",
    "            # get the y_hat given that batch\n",
    "            y_hat = self.forward_pass(batch)\n",
    "            Y_hat.append(tf.transpose(y_hat))\n",
    "        # concatenate all the predicted values \n",
    "        Y_hat = tf.concat(Y_hat, axis = 0)\n",
    "        return Y_hat\n",
    "     \n",
    "    def evaluate(self, X_batches, Y, threshold = 0.7):\n",
    "        \"\"\"\n",
    "        evaluate the model with the current parameters \n",
    "        inputs:\n",
    "        X_batches (list)-------> list of tensors each tensor has size [d,n]\n",
    "        Y         (Tensor)-----> true labels from the data \n",
    "        parameters (tuple)-----> tuple contain encoder parameters and Head parameters respectively\n",
    "        threshold  (float)-----> the threshold to approximate the probabilities from y_hat\n",
    "        \"\"\"\n",
    "        Y_hat = self.predict(X_batches)\n",
    "        # calculate the loss between the true y and the predicted y\n",
    "        loss  = tf.compat.v2.losses.binary_crossentropy(Y, Y_hat)\n",
    "        average_loss = tf.reduce_mean(loss)\n",
    "        # calculate the accuarcy between the true y and the predicted y\n",
    "        # don't forget to approximate the probabilities using the threshold\n",
    "        Y_hat = tf.where(Y_hat > threshold, 1,0)\n",
    "        acc = tf.metrics.Accuracy()\n",
    "        acc.update_state(Y, Y_hat)\n",
    "        average_acc = acc.result().numpy()\n",
    "\n",
    "        return average_loss,average_acc\n",
    "    def get_batch(self, df_text, df_labels, st, batch_size):\n",
    "        \"\"\"\n",
    "        this function deals with datafram directly\n",
    "        inputs:\n",
    "        df_text   (datafram)-------> datafram contains text comments\n",
    "        df_labels (datafram)-------> datafram contains all the labels\n",
    "        st        (int)------------> start index of the batch\n",
    "        batch_size(int)------------> number of batchs\n",
    "        output:\n",
    "        X_batces,Y_batches(tuple)--> batches of embeddings and labels\n",
    "        \"\"\"\n",
    "        # access the batches then turn them to numpy array\n",
    "        sentences = df_text.iloc[st:st+batch_size].values\n",
    "        X_batces = []\n",
    "        # store the embeddings of each sentence in a list \n",
    "        for sentence in sentences:\n",
    "            x = self.input_embeddings(sentence)\n",
    "            # x dim is 300 x 50\n",
    "            # we needto stack or concat them\n",
    "            X_batces.append(x)\n",
    "        Y_batches = df_labels.iloc[st:st+batch_size,:].values\n",
    "        # note X_batches is a list\n",
    "        return X_batces,Y_batches\n",
    "    def train(self, text, labels, epochs, alpha, b, stop_early):\n",
    "        \"\"\"\n",
    "        train the model given the data\n",
    "        inputs :\n",
    "        text       (datafram)-----> datafram column contains tokenized sentence \n",
    "        labels     (datafram)-----> datafram columns contain all the labels\n",
    "        epochs     (int)----------> number of epochs to train the model\n",
    "        b          (int)----------> number of batchs\n",
    "        stop_early (tuple)--------> accuracy,loss thresholds to stop training\n",
    "        output:\n",
    "        parameters (tuple)--------> tuple of all the parameters of the encoder and the head after training \n",
    "        \"\"\"\n",
    "        # define the steps required to train all the batches\n",
    "        steps = math.floor(len(text)/b)\n",
    "        # define an adam optimizer to update the parameters\n",
    "        # clip norm is used to avoid gradients explosion\n",
    "        opt  = tf.compat.v2.optimizers.Adam(learning_rate = alpha,clipnorm = 1.0)\n",
    "        # Define a list of accuracies calculated at each step and then averaged at the end  of each epoch\n",
    "        accu_per_epoch = [0]\n",
    "        for epoch in range(epochs) :\n",
    "            # stop if we have reached desired accurcy\n",
    "            if (np.mean(accu_per_epoch) > stop_early[0]) and (np.mean(loss_per_epoch) < stop_early[1]):\n",
    "                break\n",
    "            loss_per_epoch = []\n",
    "            accu_per_epoch = []\n",
    "            s = 1\n",
    "            # loop and get a batch then update the parameters based on the loss \n",
    "            for st in range(0,len(text),b):\n",
    "                X_batches,Y = self.get_batch(text, labels, st, b)\n",
    "                # gradient tape is very effective method to get the gradients of the parameters\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(self.parameters)\n",
    "                    loss,accu   = self.evaluate(X_batches, Y)\n",
    "                    accu_per_epoch.append(accu)\n",
    "                    loss_per_epoch.append(loss)\n",
    "\n",
    "                gradients = tape.gradient(loss, self.parameters)\n",
    "                # update the model using adam optimizer with the gradients\n",
    "                opt.apply_gradients(zip(gradients, self.parameters))\n",
    "                # update and print the results at each step\n",
    "                sys.stdout.write(\"\\r\"+\"Epoch : \"+str(epoch+1)+\\\n",
    "                                 \"  step : \"+str(s)+\"\\\\\"+str(steps)+\\\n",
    "                                 \"  loss : \"+str(np.mean(loss_per_epoch))+\\\n",
    "                                 \"  accuracy : \"+str(np.mean(accu_per_epoch)))\n",
    "                s += 1\n",
    "            print(\"\")\n",
    "    def test(self, text, labels, b):\n",
    "        # test function works the same way as train  except it doesn't update the parameters \n",
    "        steps = math.floor(len(text)/b)\n",
    "        loss_per_epoch = []\n",
    "        accu_per_epoch = []\n",
    "        s = 1\n",
    "        for st in range(0,len(text),b):\n",
    "            X_batches,Y = self.get_batch(text, labels, st, b)    \n",
    "            loss,accu   = self.evaluate(X_batches, Y)\n",
    "            accu_per_epoch.append(accu)\n",
    "            loss_per_epoch.append(loss)\n",
    "            sys.stdout.write(\"\\r\"+\"Epoch : 1\"+\\\n",
    "                             \"  step : \"+str(s)+\"\\\\\"+str(steps)+\\\n",
    "                             \"  test_loss : \"+str(np.mean(loss_per_epoch))+\\\n",
    "                             \"  test_accuracy : \"+str(np.mean(accu_per_epoch)))\n",
    "            s += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.import_parameters(\"parameters.pk1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data before dividing it into train and test\n",
    "df = df.sample(frac = 1, random_state = 1).reset_index(drop=True)\n",
    "# define a splitter to split the data into 60% training and the reset for testing\n",
    "s = int(len(df)*0.6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(df.iloc[s:,1],df.iloc[s:,2:],256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style = \"font-size: 37px\">Phases</p>\n",
    "<br>\n",
    "<ul style = \"font-size : 25px;\">\n",
    "<li ><a href = \"#Loading the data\">Loading the data</a></li>\n",
    "<br/>    \n",
    "<li><a href = \"#Data preprocessing\">Data preprocessing</a></li>\n",
    "<br/>      \n",
    "<li><a href = \"#Model Building\">Model Building</a></li>       \n",
    "</ul>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "<p id = \"Loading the data\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                                  # use regular expression to deal with text\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize     # for word and sentence tokenization\n",
    "from nltk.corpus import stopwords                          # to help removing stopping words\n",
    "from nltk.stem import WordNetLemmatizer                    # to lemmatize the words\n",
    "from string import punctuation                             # to remove punctuation like ? ! . etc\n",
    "import io                                                  # to read embeddings from text file\n",
    "import math                                                # to get mathmatical functions sin cos etc\n",
    "import pandas as pd                                        # to deal with dataframes    \n",
    "import numpy as np                                         # to deal with numbers and dataframes  \n",
    "import tensorflow as tf                                    # the main package to build the transformer\n",
    "import sys                                                 # good printing tool\n",
    "import pickle                                              # importing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red the data as datafram\n",
    "df = pd.read_csv(r\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the embeddings as dictionary where key is the word and value it's corresponding embbeding\n",
    "def load_vectors(fname):\n",
    "    \"\"\"\n",
    "    this function takes the path of the embeddings file 'must be from fast text'\n",
    "    and returns a dictionary where the words are keys and thier embeddings as values\n",
    "    input :\n",
    "    fname (String) ------> file path\n",
    "    output :\n",
    "    data (dictionary)----> dictionary to acess the embeddings with words as keys\n",
    "    \"\"\"\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] =  tokens[1:] \n",
    "    # add pad token as zero vector\n",
    "    # we will expalain this later\n",
    "    data[\"<PAD>\"] = [0]*300    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = load_vectors(\"wiki.simple.vec.text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "<p id = \"Data preprocessing\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleansing(text,maximum_len = 20):\n",
    "    \"\"\"\n",
    "     This function performs the following steps:\n",
    "\n",
    "    1. Lowercases the text\n",
    "    2. Removes HTTP and WWW links\n",
    "    3. Removes non-ASCII characters\n",
    "    4. Removes numbers\n",
    "    5. Tokenizes the text\n",
    "    6. Removes stop words\n",
    "    7. Lemmatizes the tokens\n",
    "    8. Truncates tokens that are longer than the specified maximum length\n",
    "    inputs :\n",
    "    text (String) -----------> String to perform the cleaning\n",
    "    maximum_len(int)--------->The maximum length of tokens to keep\n",
    "    output:\n",
    "    cleaned_tokens(text)----->A string containing the cleaned text\n",
    "\n",
    "    \"\"\"\n",
    "    # lower case the text\n",
    "    text = text.lower()\n",
    "    # replace the text that have a ppatern of http link with empty link\n",
    "    text = re.sub(r\"(https?[\\s+]?:?[\\s+]?[(\\w+|.)/?(\\w+|.)?]+[\\s]?)\",\"\",text)\n",
    "    # replace the text that have a ppatern of only www. link\n",
    "    text = re.sub(r\"(\\w+[\\s+]?:[\\s+]?www.\\w+.\\w+)\",\"\",text)\n",
    "    # remove none ASCII characters like \"| ¡ ¿ † ‡ ↔ ↑ ↓ • ¶\"\n",
    "    text = re.sub(r'[^\\x00-\\x7f]','',text)\n",
    "    # remove numbers from the text \n",
    "    text = re.sub(r\"\\d\",\"\",text)\n",
    "    # turn the text into list of words \n",
    "    tokens = word_tokenize(text)\n",
    "    # cach the stopping words that in english\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    # intialize a lemmatizer \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # define a new list to keep the cleaned tokens\n",
    "    cleaned_tokens = []\n",
    "    # iterate over the tokens to remove the words that exceeds the maximum length \n",
    "    # remove stopping words\n",
    "    # remove punctuation\n",
    "    # lemmatize the words\n",
    "    for i,token in enumerate(tokens):\n",
    "        if (len(token) > maximum_len) or (token in stop_words):\n",
    "            # do nothing \n",
    "            s = \"\"\n",
    "        else:\n",
    "            s = token.strip(punctuation)\n",
    "            if s != \"\":\n",
    "                cleaned_tokens.append(lemmatizer.lemmatize(s))\n",
    "\n",
    "    return \" \".join(cleaned_tokens)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the claning to each row of the data\n",
    "df.iloc[:,1] = df.iloc[:,1].apply(text_cleansing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    this function truncate the sentences that exceeds 50 words\n",
    "    and add paddings to senteces that is shorter than 50\n",
    "    input :\n",
    "    text (String) ------> text of the sentence\n",
    "    output:\n",
    "    tokens (list) ------> list of words 'tokens'\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)  \n",
    "    if len(tokens) > 50:\n",
    "        tokens = tokens[:50]\n",
    "        return tokens\n",
    "    else :\n",
    "        padding_len = 50 - len(tokens)\n",
    "        tokens.extend([\"<PAD>\"]*padding_len)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the tokenizer to each sentence\n",
    "df.iloc[:,1] = df.iloc[:,1].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building\n",
    "<p id = \"Model Building\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, dic, n = 50, d = 300, h = 2, k = 300, c = 1200, m = 2):\n",
    "        self.dic = dic\n",
    "        self.n = n\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.m = m\n",
    "        self.positional_encoding      = self.positional_embeddings()\n",
    "        self.encoder_parameters       = self.Encoder_parameters_intializer()\n",
    "        self.head_parameters          = self.Head_parameters_intializer()\n",
    "        self.parameters               = self.encoder_parameters + self.head_parameters\n",
    "        \n",
    "    def import_parameters(self,pname):\n",
    "        with open(pname, 'rb') as file:\n",
    "            # Call load method to deserialze\n",
    "            self.parameters = pickle.load(file)\n",
    "        self.encoder_parameters = self.parameters[:8]\n",
    "        self.head_parameters    = self.parameters[8:]\n",
    "        \n",
    "    def input_embeddings(self, sentence):\n",
    "        \"\"\"\n",
    "        Converts a sentence into a matrix of input embeddings of dim d x n\n",
    "        d is embeddings length \n",
    "        n is the sentence length\n",
    "        input :\n",
    "        sentence (list) ------> list of words\n",
    "        dic (dictionary)------> dictionary of embeddings\n",
    "        output :\n",
    "        tokens (Tensor)-------> tensor of size [d , n]\n",
    "\n",
    "        \"\"\"\n",
    "        dic = self.dic\n",
    "        \n",
    "        tokens = []\n",
    "        for word in sentence:\n",
    "            if word in dic:\n",
    "                tokens.append(dic[word])\n",
    "            # if the word out-of-vocab use special token </s>    \n",
    "            else :\n",
    "                tokens.append(dic['</s>'])\n",
    "        # this make sure that the array in size (d,n)        \n",
    "        tokens =  np.array(tokens, dtype = np.float32).T \n",
    "        return tf.Variable(tokens)      \n",
    "    def positional_embeddings(self, c = 10000):\n",
    "        \"\"\"\n",
    "        get the positional embeddings of embeddings of size [d,n]\n",
    "        inputs :\n",
    "        d (int) -------> is the diemnsion of the embeddings\n",
    "        n (int) -------> is the number of tokens in the sentence\n",
    "        c (int) -------> hyperparameter by default 10000\n",
    "        output :\n",
    "        tokens (Tensor)------> positional embeddings for the tokens\n",
    "        \"\"\"\n",
    "        d = self.d\n",
    "        n = self.n\n",
    "        \n",
    "        tokens = []\n",
    "        for position in range(n):\n",
    "            tokens.append([])\n",
    "            for i in range(d):\n",
    "                # if i is even calculate even_pe\n",
    "                if i%2 == 0:\n",
    "                    even_d    = math.pow(c, i/d)\n",
    "                    even_pe = math.sin(position/ even_d)\n",
    "                    tokens[position].append(even_pe)\n",
    "                # if i is odd calculate odd_pe    \n",
    "                else:\n",
    "                    odd_d    = math.pow(c, i/d)\n",
    "                    odd_pe = math.cos(position/ odd_d)\n",
    "                    tokens[position].append(odd_pe)            \n",
    "        return tf.Variable(np.array(tokens).T, dtype = tf.float32, shape = [d,n] )\n",
    "    def padding_mask(self, X):\n",
    "        \"\"\"\n",
    "        Create mask which marks the zero padding values in the input by a 1\n",
    "        this mask is used to force the model to ignore paddings\n",
    "        inputs:\n",
    "        X (Tensor) -------> matrix of embeddings of size [d,n]\n",
    "        n (int)    -------> number of words in the sentences\n",
    "        output:\n",
    "        mask (Tesnor)----> mask with the same size as X\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        \n",
    "        # get boolean vector to indicate position of padding  \n",
    "        mask = tf.math.equal(X, 0)\n",
    "        mask = tf.reduce_all(mask, axis = 0)\n",
    "        # cast the vector from boolean to float32 then rshape it as column vector\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        mask = tf.reshape(mask, (1,-1))\n",
    "        # repeat that column n times \n",
    "        mask = tf.concat([mask]*n, axis = 0)\n",
    "        return mask\n",
    "    def MHSA(self, X, U_q, U_k, V, W):\n",
    "        \"\"\"\n",
    "        this function return multi head self attention tensor given the parameters and input matrix X\n",
    "        inputs :\n",
    "        X   (Tensor)----> input matrix of size [d,n]\n",
    "        U_q (Tensor)---> matrix of size [h,k,d] is used to get Query matrix\n",
    "        U_k (Tensor)---> matrix of size [h,k,d] is used to get Key matrix\n",
    "        V   (Tensor)---> projection matrix of size [h,d,d]\n",
    "        W   (Tensor)---> weight matrix with size [d,h*d] to project the concatenated heads back to X size\n",
    "        h   (int)------> number of heads\n",
    "        k   (int)------> dimension of Query,Key matrixes\n",
    "        output :\n",
    "        mhsa (Tensor)----> matrix of size [d,n]\n",
    "        \"\"\"\n",
    "        d   = self.d\n",
    "        n   = self.n\n",
    "        h   = self.h\n",
    "        k   = self.k\n",
    "        \n",
    "        Q   = tf.matmul(U_q, X)  \n",
    "        K   = tf.matmul(U_k, X)\n",
    "        m   = self.padding_mask(X) * -1e9 \n",
    "        A   = tf.nn.softmax((tf.matmul(Q, K, transpose_a = True) + m)  / np.sqrt(k), axis= 1)\n",
    "        Y    = V @ X @ A\n",
    "        mhsa = W @ tf.reshape(Y, [h*d,n])\n",
    "        return mhsa\n",
    "    def Add_Norm(self, x1, x2):\n",
    "        \"\"\"\n",
    "        simple function to add two tensors and then normalize the resultant tensor over their rows\n",
    "        inputs :\n",
    "        x1 (Tensor)-----> tensor of size [d,n]\n",
    "        x2 (Tensor)-----> tensor of size [d,n]\n",
    "        output:\n",
    "        Normalized_X (Tenosr)-----> normalized tensor of size [d,n]\n",
    "        \"\"\"\n",
    "        X = x1 + x2\n",
    "        mean = tf.math.reduce_mean(X,axis = 1)\n",
    "        mean = tf.reshape(mean, [-1,1])\n",
    "        std = tf.math.reduce_std(X,axis = 1)\n",
    "        std = tf.reshape(std, [-1,1]) +1e-6\n",
    "        Normalized_X = (X - mean) / std\n",
    "        return Normalized_X\n",
    "    def feed_forward_NN(self,X, W1, W2, b1, b2):\n",
    "        # c approxamatly 4 * d\n",
    "        # in the paper d = 512 and c = 2048\n",
    "        hidden_layer = tf.nn.relu(W1 @ X + b1)\n",
    "        Dropout      = tf.nn.dropout(hidden_layer,0.5)\n",
    "        output       = W2 @ Dropout + b2\n",
    "        return output\n",
    "    def Encoder_parameters_intializer(self):\n",
    "        # this function just intialize encoder parameters\n",
    "        # All the distributions have been chosen based on a lot of experiments\n",
    "        h = self.h\n",
    "        k = self.k\n",
    "        d = self.d\n",
    "        c = self.c\n",
    "        \n",
    "        U_q = tf.random.uniform((h,k,d), minval = 0, maxval = 1)\n",
    "        U_q = tf.Variable(U_q)\n",
    "        U_k = tf.random.uniform((h,k,d), minval = 0, maxval = 1)\n",
    "        U_k = tf.Variable(U_k)\n",
    "        V   = tf.random.uniform((h,d,d), minval = 0, maxval = 1)\n",
    "        V = tf.Variable(V)\n",
    "        W   = tf.random.uniform((d,h*d), minval = 0, maxval = 1)\n",
    "        W = tf.Variable(W)\n",
    "        W1  = tf.random.normal((c,d))\n",
    "        W1 = tf.Variable(W1)\n",
    "        W2  = tf.random.normal((d,c))\n",
    "        W2 = tf.Variable(W2)\n",
    "        b1  = tf.random.uniform((c,1))\n",
    "        b1 = tf.Variable(b1)\n",
    "        b2  = tf.random.uniform((d,1))\n",
    "        b2 = tf.Variable(b2)\n",
    "        return U_q,U_k,V,W,W1,W2,b1,b2\n",
    "    \n",
    "    def Encoder(self, X):\n",
    "        # we have described the function in details through the mathematical notation above\n",
    "        U_q,U_k,V,W,W1,W2,b1,b2 = self.encoder_parameters\n",
    "        X   = X + self.positional_encoding\n",
    "        X    = tf.nn.dropout(X,0.1)\n",
    "        mhsa = self.MHSA(X, U_q, U_k, V, W)\n",
    "        X    = self.Add_Norm(X, mhsa)\n",
    "        fnn  = self.feed_forward_NN(X, W1, W2, b1, b2)\n",
    "        X    = self.Add_Norm(X, fnn)\n",
    "        \n",
    "    def Transformer_block(self, X):\n",
    "        \"\"\"\n",
    "        this function combine multiple layers of the encoder to get transformer output\n",
    "        inputs :\n",
    "        X          (Tensor)-----> tesnor of the embeddings of size [d,n]\n",
    "        parameters (tuple)------> tuple of encoder parameters\n",
    "        output:\n",
    "        encoder_output (Tensor)---> the output of the transformer which has the same size as X\n",
    "        \"\"\"\n",
    "        # define one layer outside the loop then use the output as input\n",
    "        # repeat the process m-1 times \n",
    "        # don't forget the dropout to prevent overfitting\n",
    "        encoder_output = self.Encoder(X)\n",
    "        encoder_output = tf.nn.dropout(encoder_output,0.1)\n",
    "        for i in range(self.m-1):\n",
    "            encoder_output = self.Encoder(encoder_output)\n",
    "            encoder_output = tf.nn.dropout(encoder_output,0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
